{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "BOwsuGQQY9OL",
    "outputId": "9ac5f7fc-ad0d-4552-f28c-82e98cc0745a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "### YOUR CODE HERE\n",
    "\n",
    "import tensorflow.keras.regularizers as reg\n",
    "\n",
    "###\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv",
    "outputId": "dad4c5ae-4e92-4d2f-e1fb-b1e857955f2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./tmp/sonnets.txt: No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tmp/sonnets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0dbb05e533ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wget --no-check-certificate      https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt      -O ./tmp/sonnets.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tmp/sonnets.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tmp/sonnets.txt'"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "#!wget --no-check-certificate \\\n",
    "#    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
    "#    -O ./tmp/sonnets.txt\n",
    "data = open('./tmp/sonnets.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "w9vH8Y59ajYL",
    "outputId": "27ed9022-67e9-4d75-d7b8-b2979c208f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 300)               541200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3211)              966511    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3211)              10313732  \n",
      "=================================================================\n",
      "Total params: 12,443,743\n",
      "Trainable params: 12,443,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words,\n",
    "                kernel_regularizer=reg.l2(0.01),\n",
    "                activity_regularizer=reg.l1(0.01),\n",
    "                activation='relu'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "AIg2f1HBxqof",
    "outputId": "830c0467-99f5-47bf-c68e-0cc6b2913b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 34s 2ms/sample - loss: 7.0558 - acc: 0.0206\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.5231 - acc: 0.0244\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.4425 - acc: 0.0302\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.3927 - acc: 0.0347\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.3499 - acc: 0.0356\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.3164 - acc: 0.0363\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.2910 - acc: 0.0389\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.2530 - acc: 0.0389\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.2115 - acc: 0.0419\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.1619 - acc: 0.0422\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.1255 - acc: 0.0436\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 6.0966 - acc: 0.0441\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.0638 - acc: 0.0479\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.0439 - acc: 0.0470\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.0178 - acc: 0.0490\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 32s 2ms/sample - loss: 5.9972 - acc: 0.0468\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 5.9700 - acc: 0.0493\n",
      "Epoch 18/100\n",
      "11360/15462 [=====================>........] - ETA: 8s - loss: 5.9399 - acc: 0.0446"
     ]
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fXTEO3GJ282"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-Week4-Exercise-Shakespeare-Question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
